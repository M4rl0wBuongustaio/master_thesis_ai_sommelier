{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTModelForFeatureExtraction\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from models import pipeline_onnx_transformers as pipe\n",
    "from transformers import AutoTokenizer\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "model_st = SentenceTransformer.load('../models/zero-shot')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "test_list = ['This is a test string for testing embedding speed.'] * 1000"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time for 1.000 strings:  0:00:01.397364\n",
      "Median time for 1.000 strings:  0:00:01.349685\n"
     ]
    }
   ],
   "source": [
    "duration_list = list()\n",
    "for i in range(100):\n",
    "    start = datetime.now()\n",
    "    model_st.encode(test_list, convert_to_tensor=True)\n",
    "    duration = datetime.now() - start\n",
    "    duration_list.append(duration)\n",
    "\n",
    "print('Average time for 1.000 strings: ', str(np.mean(duration_list)))\n",
    "print('Median time for 1.000 strings: ', str(np.median(duration_list)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "onnx_model = ORTModelForFeatureExtraction.from_pretrained('../models/zero-shot', from_transformers=True)\n",
    "onnx_tokenizer = AutoTokenizer.from_pretrained('../models/zero-shot')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "vanilla_emb = pipe.SentenceEmbeddingPipeline(model=onnx_model, tokenizer=onnx_tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time for 1.000 strings:  0:00:02.091777\n",
      "Median time for 1.000 strings:  0:00:02.031237\n"
     ]
    }
   ],
   "source": [
    "duration_list = list()\n",
    "for i in range(10):\n",
    "    start = datetime.now()\n",
    "    vanilla_emb(test_list)\n",
    "    duration = datetime.now() - start\n",
    "    duration_list.append(duration)\n",
    "\n",
    "print('Average time for 1.000 strings: ', str(np.mean(duration_list)))\n",
    "print('Median time for 1.000 strings: ', str(np.median(duration_list)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:02.085814\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "vanilla_emb(test_list)\n",
    "print(datetime.now() - start)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OptimizationConfig' object has no attribute 'use_multi_head_attention'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 9\u001B[0m\n\u001B[1;32m      6\u001B[0m optimization_config \u001B[38;5;241m=\u001B[39m OptimizationConfig(optimization_level\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m99\u001B[39m) \u001B[38;5;66;03m# enable all optimizations\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# apply the optimization configuration to the model\u001B[39;00m\n\u001B[0;32m----> 9\u001B[0m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimize\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43msave_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m../models/onnx\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptimization_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimization_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.10/site-packages/optimum/onnxruntime/optimization.py:128\u001B[0m, in \u001B[0;36mORTOptimizer.optimize\u001B[0;34m(self, optimization_config, save_dir, file_suffix, use_external_data_format)\u001B[0m\n\u001B[1;32m    126\u001B[0m model_type \u001B[38;5;241m=\u001B[39m ORTConfigManager\u001B[38;5;241m.\u001B[39mget_model_ort_type(model_type)\n\u001B[1;32m    127\u001B[0m optimization_config\u001B[38;5;241m.\u001B[39mmodel_type \u001B[38;5;241m=\u001B[39m model_type\n\u001B[0;32m--> 128\u001B[0m optimization_options \u001B[38;5;241m=\u001B[39m \u001B[43mFusionOptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparse\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptimization_config\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    129\u001B[0m LOGGER\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptimizing model...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    131\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m model_path \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39monnx_model_path:\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.10/site-packages/onnxruntime/transformers/fusion_options.py:76\u001B[0m, in \u001B[0;36mFusionOptions.parse\u001B[0;34m(args)\u001B[0m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m args\u001B[38;5;241m.\u001B[39mdisable_attention:\n\u001B[1;32m     75\u001B[0m     options\u001B[38;5;241m.\u001B[39menable_attention \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m---> 76\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muse_multi_head_attention\u001B[49m:\n\u001B[1;32m     77\u001B[0m     options\u001B[38;5;241m.\u001B[39muse_multi_head_attention \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m args\u001B[38;5;241m.\u001B[39mdisable_skip_layer_norm:\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'OptimizationConfig' object has no attribute 'use_multi_head_attention'"
     ]
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTOptimizer\n",
    "from optimum.onnxruntime.configuration import OptimizationConfig\n",
    "\n",
    "# create ORTOptimizer and define optimization configuration\n",
    "optimizer = ORTOptimizer.from_pretrained(onnx_model)\n",
    "optimization_config = OptimizationConfig(optimization_level=99) # enable all optimizations\n",
    "\n",
    "# apply the optimization configuration to the model\n",
    "optimizer.optimize(\n",
    "    save_dir='../models/onnx',\n",
    "    optimization_config=optimization_config,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "ename": "NoSuchFile",
     "evalue": "[ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from ../models/onnx/model.onnx failed:Load model ../models/onnx/model.onnx failed. File doesn't exist",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNoSuchFile\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m onnx_model \u001B[38;5;241m=\u001B[39m \u001B[43mORTModelForFeatureExtraction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m../models/onnx\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.10/site-packages/optimum/onnxruntime/modeling_ort.py:252\u001B[0m, in \u001B[0;36mORTModel.from_pretrained\u001B[0;34m(cls, model_id, from_transformers, force_download, use_auth_token, cache_dir, provider, session_options, provider_options, *args, **kwargs)\u001B[0m\n\u001B[1;32m    224\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[1;32m    225\u001B[0m \u001B[38;5;129m@add_start_docstrings\u001B[39m(FROM_PRETRAINED_START_DOCSTRING)\n\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfrom_pretrained\u001B[39m(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    237\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    238\u001B[0m ):\n\u001B[1;32m    239\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    240\u001B[0m \u001B[38;5;124;03m    provider (`str`, *optional*):\u001B[39;00m\n\u001B[1;32m    241\u001B[0m \u001B[38;5;124;03m        ONNX Runtime providers to use for loading the model. See https://onnxruntime.ai/docs/execution-providers/ for\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    250\u001B[0m \u001B[38;5;124;03m        `ORTModel`: The loaded ORTModel model.\u001B[39;00m\n\u001B[1;32m    251\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 252\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    253\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    254\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfrom_transformers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    255\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    256\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_auth_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    257\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    258\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprovider\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprovider\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    259\u001B[0m \u001B[43m        \u001B[49m\u001B[43msession_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msession_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    260\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprovider_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprovider_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    261\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    262\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    263\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.10/site-packages/optimum/modeling_base.py:241\u001B[0m, in \u001B[0;36mOptimizedModel.from_pretrained\u001B[0;34m(cls, model_id, from_transformers, force_download, use_auth_token, cache_dir, **model_kwargs)\u001B[0m\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_from_transformers(\n\u001B[1;32m    233\u001B[0m         model_id\u001B[38;5;241m=\u001B[39mmodel_id,\n\u001B[1;32m    234\u001B[0m         revision\u001B[38;5;241m=\u001B[39mrevision,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    238\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[1;32m    239\u001B[0m     )\n\u001B[1;32m    240\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 241\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_from_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    242\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    243\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    244\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    245\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    246\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_auth_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_auth_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    247\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    248\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.10/site-packages/optimum/onnxruntime/modeling_ort.py:306\u001B[0m, in \u001B[0;36mORTModel._from_pretrained\u001B[0;34m(cls, model_id, use_auth_token, revision, force_download, cache_dir, file_name, **kwargs)\u001B[0m\n\u001B[1;32m    304\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misdir(model_id):\n\u001B[1;32m    305\u001B[0m     config \u001B[38;5;241m=\u001B[39m PretrainedConfig\u001B[38;5;241m.\u001B[39mfrom_dict(config_dict)\n\u001B[0;32m--> 306\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mORTModel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_file_name\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    307\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_save_dir\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m Path(model_id)\n\u001B[1;32m    308\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlatest_model_name\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m model_file_name\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.10/site-packages/optimum/onnxruntime/modeling_ort.py:200\u001B[0m, in \u001B[0;36mORTModel.load_model\u001B[0;34m(path, provider, session_options, provider_options, **kwargs)\u001B[0m\n\u001B[1;32m    197\u001B[0m     providers\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCUDAExecutionProvider\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    199\u001B[0m \u001B[38;5;66;03m# `providers` list must of be of the same length as `provider_options` list\u001B[39;00m\n\u001B[0;32m--> 200\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mort\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mInferenceSession\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    201\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    202\u001B[0m \u001B[43m    \u001B[49m\u001B[43mproviders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproviders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    203\u001B[0m \u001B[43m    \u001B[49m\u001B[43msess_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msession_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    204\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprovider_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mprovider_options\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43mprovider_options\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    205\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:360\u001B[0m, in \u001B[0;36mInferenceSession.__init__\u001B[0;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001B[0m\n\u001B[1;32m    357\u001B[0m disabled_optimizers \u001B[38;5;241m=\u001B[39m kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdisabled_optimizers\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdisabled_optimizers\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m kwargs \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    359\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 360\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_inference_session\u001B[49m\u001B[43m(\u001B[49m\u001B[43mproviders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprovider_options\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdisabled_optimizers\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    361\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n\u001B[1;32m    362\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_enable_fallback:\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:397\u001B[0m, in \u001B[0;36mInferenceSession._create_inference_session\u001B[0;34m(self, providers, provider_options, disabled_optimizers)\u001B[0m\n\u001B[1;32m    395\u001B[0m session_options \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sess_options \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sess_options \u001B[38;5;28;01melse\u001B[39;00m C\u001B[38;5;241m.\u001B[39mget_default_session_options()\n\u001B[1;32m    396\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_model_path:\n\u001B[0;32m--> 397\u001B[0m     sess \u001B[38;5;241m=\u001B[39m \u001B[43mC\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mInferenceSession\u001B[49m\u001B[43m(\u001B[49m\u001B[43msession_options\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_model_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_read_config_from_model\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    398\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    399\u001B[0m     sess \u001B[38;5;241m=\u001B[39m C\u001B[38;5;241m.\u001B[39mInferenceSession(session_options, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_model_bytes, \u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_read_config_from_model)\n",
      "\u001B[0;31mNoSuchFile\u001B[0m: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from ../models/onnx/model.onnx failed:Load model ../models/onnx/model.onnx failed. File doesn't exist"
     ]
    }
   ],
   "source": [
    "onnx_model = ORTModelForFeatureExtraction.from_pretrained('../models/onnx')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
